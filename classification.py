# -*- coding: utf-8 -*-
"""classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1j6H3yxCtwWRHfYSZ5tLr5wC-rc8R9axt

씨앗의 품종에 대해 라벨링된 데이터를 가진 특성으로 분류한다.
https://archive.ics.uci.edu/dataset/236/seeds
1. area A, 씨앗의 면적
2. perimeter P,씨앗의 둘레
3. compactness C = 4*pi*A/P^2,조밀도
4. length of kernel,씨앗의 길이
5. width of kernel,씨앗의 너비
6. asymmetry coefficient 비대칭 계수
7. length of kernel groove.씨앗 홈의 길이
씨앗 데이터

Seeds 데이터셋은 세 가지 밀 품종(Kama, Rosa, Canadian)의 210개의 샘플로 구성되어 있다. Polish Academy of Sciences에서 수집되었으며, 연부 X선(Soft X-ray) 기술을 활용해 비파괴적으로 씨앗 내부 구조를 촬영했다. 데이터는 면적, 둘레, 콤팩트함, 길이, 너비, 비대칭 계수, 홈의 길이 등 7가지 특성을 포함한다

라벨이 마지막 열
"""

# 자료구조
import pandas as pd
import numpy as np
# normalize
from sklearn.preprocessing import StandardScaler
# 분류기
from sklearn import tree, ensemble, svm
# 분류 성능평가용
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.metrics import precision_score, recall_score, f1_score
# 데이터 시각화용
from matplotlib import pyplot as plt
from sklearn.decomposition import PCA
import seaborn as sns
from mpl_toolkits.mplot3d import Axes3D
from matplotlib.colors import ListedColormap

# 파라미터 조정
test_rate =0.8
randseed = 42
np.random.seed(randseed)
cri = "\t"
file_path = r"./data.txt"
# 분류 옵션
modellist=["decision tree","random forest","adaboost","SVM"]
# 스케일조정옵션
isScaling = True


# 파일에서 데이터 로드
# file_path, cri -> df
match(file_path.split(".")[-1].lower()):
    case "txt":
        df = pd.read_csv(file_path, delimiter=cri, skipinitialspace=True)
    case "csv":
        df = pd.read_csv(file_path, delimiter=cri, skipinitialspace=True)
    case "xlsx":
        df = pd.read_excel(file_path)
    case _:
        raise ValueError("Unsupported file type")


# 데이터 형태 파악
print(df.head())
pca = PCA(n_components=3)
X_pca = pca.fit_transform(df.iloc[:,:-1])
lab = df.iloc[:,-1]
fig = plt.figure(figsize=(10, 7))
ax = fig.add_subplot(111, projection='3d')
custom_colors = ['red', 'blue', 'green']
cmap = ListedColormap(custom_colors)
scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=lab, cmap=cmap, alpha=0.7)
ax.set_xlabel("PCA Component 1")
ax.set_ylabel("PCA Component 2")
ax.set_zlabel("PCA Component 3")
ax.set_title("3D PCA Visualization")


# normalize
if isScaling:
    features = df.drop(df.columns[-1], axis=1)
    scaler = StandardScaler()
    scaled_features = scaler.fit_transform(features)
    df_scaled = pd.DataFrame(scaled_features, columns=features.columns)
    df_scaled['wheatSpecies'] = df['wheatSpecies']
    print("Scaled data\n",df_scaled.head())
    df = df_scaled


# 섞고 test data 분리
# df, test_rate -> train_data, test_data,train_size
data_size = len(df)
train_size = int(test_rate * data_size)
indice = np.arange(data_size)
np.random.shuffle(indice)

train_idx = indice[:train_size]
test_idx = indice[train_size:]

train_data = df.iloc[train_idx]
test_data = df.iloc[test_idx]
#라벨 분리
train_labels = train_data.iloc[:,-1]
test_labels = test_data.iloc[:,-1]
train_data = train_data.iloc[:,:-1]
test_data = test_data.iloc[:,:-1]


# 분류실행/결과 출력까지
# train_data,train_labels ,test_data, modellist-> predicted
# predicted, test_labels -> 결과 출력
for model in modellist:
    match(model):
        case "decision tree":
            classifier = tree.DecisionTreeClassifier(max_depth=3)
        case "random forest":
            classifier = ensemble.RandomForestClassifier(n_estimators=20, random_state=42)
        case "adaboost":
            classifier = ensemble.AdaBoostClassifier(estimator = tree.DecisionTreeClassifier(max_depth=3),n_estimators=20, random_state=42)
        case "SVM":
            classifier = svm.SVC(C = 1.0, gamma = 0.001)
        case _:
            raise ValueError("Unsupported model")

    classifier.fit(train_data, train_labels)
    predicted = classifier.predict(test_data)

    expected = test_labels
    print(15*"-")
    print(model,"\n random seed:",randseed," test_rate:",test_rate)
    print('Accuracy:',accuracy_score(expected, predicted))
    print('Confusion matrix:\n', confusion_matrix(expected, predicted))
    print("각 클래스의 점수")
    print("precision:",precision_score(expected, predicted, average=None))
    print("recall:",recall_score(expected, predicted, average=None))
    print("f-measure:",f1_score(expected, predicted, average=None))
    print("\n 평균점수")
    print("precision:",precision_score(expected, predicted, average='macro'))
    print("recall:",recall_score(expected, predicted, average='macro'))
    print("f-measure:",f1_score(expected, predicted, average='macro'))
    print(15*"-")

"""씨앗 분류 바이너리 1번과 2번 씨앗 비교"""

# 자료구조
import pandas as pd
import numpy as np
# 분류기
from sklearn import tree, ensemble, svm
# normalize
from sklearn.preprocessing import StandardScaler
# 분류 성능평가용
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.metrics import precision_score, recall_score, f1_score
# 데이터 시각화용
from matplotlib import pyplot as plt
from sklearn.decomposition import PCA
import seaborn as sns
from mpl_toolkits.mplot3d import Axes3D
from matplotlib.colors import ListedColormap

# 파라미터 조정
test_rate =0.8
randseed = 42
np.random.seed(randseed)
cri = "\t"
file_path = r"./data.txt"
# 분류 옵션
modellist=["decision tree","random forest","adaboost","SVM"]
# 스케일조정옵션
isScaling = True


# 파일에서 데이터 로드
# file_path, cri -> df
match(file_path.split(".")[-1].lower()):
    case "txt":
        df = pd.read_csv(file_path, delimiter=cri, skipinitialspace=True)
    case "csv":
        df = pd.read_csv(file_path, delimiter=cri, skipinitialspace=True)
    case "xlsx":
        df = pd.read_excel(file_path)
    case _:
        raise ValueError("Unsupported file type")

# 바이너리를 위해 씨앗 종류 3번을 제거
df = df[df.iloc[:,-1]!=3]

# normalize
if isScaling:
    features = df.drop(df.columns[-1], axis=1)
    scaler = StandardScaler()
    scaled_features = scaler.fit_transform(features)
    df_scaled = pd.DataFrame(scaled_features, columns=features.columns)
    df_scaled['wheatSpecies'] = df['wheatSpecies']
    print("Scaled data\n",df_scaled.head())
    df = df_scaled


# 섞고 test data 분리
# df, test_rate -> train_data, test_data,train_size
data_size = len(df)
train_size = int(test_rate * data_size)
indice = np.arange(data_size)
np.random.shuffle(indice)

train_idx = indice[:train_size]
test_idx = indice[train_size:]

train_data = df.iloc[train_idx]
test_data = df.iloc[test_idx]
#라벨 분리
train_labels = train_data.iloc[:,-1]
test_labels = test_data.iloc[:,-1]
train_data = train_data.iloc[:,:-1]
test_data = test_data.iloc[:,:-1]


# 분류실행/결과 출력까지
# train_data,train_labels ,test_data, modellist-> predicted
# predicted, test_labels -> 결과 출력
for model in modellist:
    match(model):
        case "decision tree":
            classifier = tree.DecisionTreeClassifier(max_depth=3)
        case "random forest":
            classifier = ensemble.RandomForestClassifier(n_estimators=20, random_state=42)
        case "adaboost":
            classifier = ensemble.AdaBoostClassifier(estimator = tree.DecisionTreeClassifier(max_depth=3),n_estimators=20, random_state=42)
        case "SVM":
            classifier = svm.SVC(C = 1.0, gamma = 0.001)
        case _:
            raise ValueError("Unsupported model")

    classifier.fit(train_data, train_labels)
    predicted = classifier.predict(test_data)

    expected = test_labels
    print(15*"-")
    print(model,"\n random seed:",randseed," test_rate:",test_rate)
    print('Accuracy:',accuracy_score(expected, predicted))
    print('Confusion matrix:\n', confusion_matrix(expected, predicted))
    print("precision:",precision_score(expected, predicted, average=None))
    print("recall:",recall_score(expected, predicted, average=None))
    print("f-measure:",f1_score(expected, predicted, average=None))
    print(15*"-")

"""의료기관종별 진료과목별 진료비 통계로 의료기관종별 분류
https://www.data.go.kr/data/15139382/fileData.do


건강보험심사평가원_의료기관종별 진료과목별 진료비 통계_2023년 데이터

341개 데이터
2023년 수집된 데이터로
의료기관종별(9종류(14종류)상급종합병원,종합병원,병원,요양병원,정신병원,의원,치과병원,보건소,보건지소,보건진료소,보건의료원,한방병원,한의원)
진료과목(내과, 일반의, ...)
환자수
명세서청구건수
입내원일수
요양급여비용총액
보험자부담금



한의원,보건소,치과의원,보건지소,보건진료소는 샘플이 1개씩이라 제외

진료과목은 너무 다양해 제외하고 환자수, 명세서청구건수,입내원일수,요양급여비용총액,보험자부담금로 의료기관종별을 분류하는 것을 목적으로함

라벨이 첫번째 열
"""

# 자료구조
import pandas as pd
import numpy as np
# normalize
from sklearn.preprocessing import StandardScaler
# 분류기
from sklearn import tree, ensemble, svm
# 분류 성능평가용
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.metrics import precision_score, recall_score, f1_score
# 데이터 시각화용
from matplotlib import pyplot as plt
from sklearn.decomposition import PCA
import seaborn as sns
from mpl_toolkits.mplot3d import Axes3D
from matplotlib.colors import ListedColormap

# 파라미터 조정
test_rate =0.8
randseed = 42
np.random.seed(randseed)
cri = ","
file_path = r"./data.csv"
# 분류 옵션
modellist=["decision tree","random forest","adaboost","SVM"]
# 스케일조정옵션
isScaling = True


# 파일에서 데이터 로드
# file_path, cri -> df
match(file_path.split(".")[-1].lower()):
    case "txt":
        df = pd.read_csv(file_path, delimiter=cri, skipinitialspace=True)
    case "csv":
        df = pd.read_csv(file_path, delimiter=cri, skipinitialspace=True)
    case "xlsx":
        df = pd.read_excel(file_path)
    case _:
        raise ValueError("Unsupported file type")


# 맞춤데이터 편집
# 진료년도, 진료과목 삭제
df = df.drop(df.columns[[0, 2]], axis=1)
# 하나씩밖에 없는 의료기관(특이케이스 삭제)
df = df.drop([269, 270,271, 272, 340],axis=0)
df = df.reset_index(drop=True)
# 의료기관종별 라벨을 0부터 시작하는 정수형 라벨로 변경
df['의료기관종별'] = pd.factorize(df['의료기관종별'])[0]


# 데이터 형태 파악
print(df.head())
pca = PCA(n_components=3)
X_pca = pca.fit_transform(df.iloc[:,1:])
lab = df.iloc[:,0]
fig = plt.figure(figsize=(10, 7))
ax = fig.add_subplot(111, projection='3d')
custom_colors = ['red', 'blue', 'green', 'yellow', 'orange', 'purple', 'brown', 'pink', 'gray']
cmap = ListedColormap(custom_colors)
scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=lab, cmap=cmap, alpha=0.7)
ax.set_xlabel("PCA Component 1")
ax.set_ylabel("PCA Component 2")
ax.set_zlabel("PCA Component 3")
ax.set_title("3D PCA Visualization")


# normalize
if isScaling:
    features = df.drop(df.columns[0], axis=1)
    scaler = StandardScaler()
    scaled_features = scaler.fit_transform(features)
    df_scaled = pd.DataFrame(scaled_features, columns=features.columns)
    df_scaled['의료기관종별'] = df['의료기관종별']
    print("Scaled data\n",df_scaled.head())
    df = df_scaled
else:
    # 노말라이즈를 하지 않아도 라벨이 마지막 열로 가게 조정
    tmp= df['의료기관종별']
    df = df.drop('의료기관종별', axis=1)
    df['의료기관종별'] = tmp


# 섞고 test data 분리
# df, test_rate -> train_data, test_data,train_size
data_size = len(df)
train_size = int(test_rate * data_size)
indice = np.arange(data_size)
np.random.shuffle(indice)

train_idx = indice[:train_size]
test_idx = indice[train_size:]

train_data = df.iloc[train_idx]
test_data = df.iloc[test_idx]
#라벨 분리
train_labels = train_data.iloc[:,-1]
test_labels = test_data.iloc[:,-1]
train_data = train_data.iloc[:,:-1]
test_data = test_data.iloc[:,:-1]


# 분류실행/결과 출력까지
# train_data,train_labels ,test_data, modellist-> predicted
# predicted, test_labels -> 결과 출력
for model in modellist:
    match(model):
        case "decision tree":
            classifier = tree.DecisionTreeClassifier(max_depth=3)
        case "random forest":
            classifier = ensemble.RandomForestClassifier(n_estimators=20, random_state=42)
        case "adaboost":
            classifier = ensemble.AdaBoostClassifier(estimator = tree.DecisionTreeClassifier(max_depth=3),n_estimators=20, random_state=42)
        case "SVM":
            classifier = svm.SVC(C = 1.0, gamma = 0.001)
        case _:
            raise ValueError("Unsupported model")

    classifier.fit(train_data, train_labels)
    predicted = classifier.predict(test_data)

    expected = test_labels
    print(15*"-")
    print(model,"\n random seed:",randseed," test_rate:",test_rate)
    print('Accuracy:',accuracy_score(expected, predicted))
    print('Confusion matrix:\n', confusion_matrix(expected, predicted))
    print("각 클래스의 점수")
    print("precision:",precision_score(expected, predicted, average=None))
    print("recall:",recall_score(expected, predicted, average=None))
    print("f-measure:",f1_score(expected, predicted, average=None))
    print("\n 평균점수")
    print("precision:",precision_score(expected, predicted, average='macro'))
    print("recall:",recall_score(expected, predicted, average='macro'))
    print("f-measure:",f1_score(expected, predicted, average='macro'))
    print(15*"-")

"""병원
바이너리 분류
가장 수가 50,50으로 많은 병원과 요양병원에 대해서만 분류를 시행해본다.
"""

# 자료구조
import pandas as pd
import numpy as np
# normalize
from sklearn.preprocessing import StandardScaler
# 분류기
from sklearn import tree, ensemble, svm
# 분류 성능평가용
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.metrics import precision_score, recall_score, f1_score
# 데이터 시각화용
from matplotlib import pyplot as plt
from sklearn.decomposition import PCA
import seaborn as sns
from mpl_toolkits.mplot3d import Axes3D
from matplotlib.colors import ListedColormap

# 파라미터 조정
test_rate =0.8
randseed = 42
np.random.seed(randseed)
cri = ","
file_path = r"./data.csv"
# 분류 옵션
modellist=["decision tree","random forest","adaboost","SVM"]
# 스케일조정옵션
isScaling = True


# 파일에서 데이터 로드
# file_path, cri -> df
match(file_path.split(".")[-1].lower()):
    case "txt":
        df = pd.read_csv(file_path, delimiter=cri, skipinitialspace=True)
    case "csv":
        df = pd.read_csv(file_path, delimiter=cri, skipinitialspace=True)
    case "xlsx":
        df = pd.read_excel(file_path)
    case _:
        raise ValueError("Unsupported file type")


# 맞춤데이터 편집
# 진료년도, 진료과목 삭제
df = df.drop(df.columns[[0, 2]], axis=1)
# 하나씩밖에 없는 의료기관(특이케이스 삭제)
df = df.drop([269, 270,271, 272, 340],axis=0)
df = df.reset_index(drop=True)
# 의료기관종별 라벨을 0부터 시작하는 정수형 라벨로 변경
df['의료기관종별'] = pd.factorize(df['의료기관종별'])[0]


#2,3 번 제외하고 모두 삭제
df = df[(df.iloc[:,0]==3)|(df.iloc[:,0]==2)]
df = df.reset_index(drop=True)


# 데이터 형태 파악
print(df.head())
pca = PCA(n_components=3)
X_pca = pca.fit_transform(df.iloc[:,1:])
lab = df.iloc[:,0]
fig = plt.figure(figsize=(10, 7))
ax = fig.add_subplot(111, projection='3d')
custom_colors = ['red', 'blue']
cmap = ListedColormap(custom_colors)
scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=lab, cmap=cmap, alpha=0.7)
ax.set_xlabel("PCA Component 1")
ax.set_ylabel("PCA Component 2")
ax.set_zlabel("PCA Component 3")
ax.set_title("3D PCA Visualization")


# normalize
if isScaling:
    features = df.drop(df.columns[0], axis=1)
    scaler = StandardScaler()
    scaled_features = scaler.fit_transform(features)
    df_scaled = pd.DataFrame(scaled_features, columns=features.columns)
    df_scaled['의료기관종별'] = df['의료기관종별']
    print("Scaled data\n",df_scaled.head())
    df = df_scaled
else:
    # 노말라이즈를 하지 않아도 라벨이 마지막 열로 가게 조정
    tmp= df['의료기관종별']
    df = df.drop('의료기관종별', axis=1)
    df['의료기관종별'] = tmp


# 섞고 test data 분리
# df, test_rate -> train_data, test_data,train_size
data_size = len(df)
train_size = int(test_rate * data_size)
indice = np.arange(data_size)
np.random.shuffle(indice)

train_idx = indice[:train_size]
test_idx = indice[train_size:]

train_data = df.iloc[train_idx]
test_data = df.iloc[test_idx]
#라벨 분리
train_labels = train_data.iloc[:,-1]
test_labels = test_data.iloc[:,-1]
train_data = train_data.iloc[:,:-1]
test_data = test_data.iloc[:,:-1]


# 분류실행/결과 출력까지
# train_data,train_labels ,test_data, modellist-> predicted
# predicted, test_labels -> 결과 출력
for model in modellist:
    match(model):
        case "decision tree":
            classifier = tree.DecisionTreeClassifier(max_depth=3)
        case "random forest":
            classifier = ensemble.RandomForestClassifier(n_estimators=20, random_state=42)
        case "adaboost":
            classifier = ensemble.AdaBoostClassifier(estimator = tree.DecisionTreeClassifier(max_depth=3),n_estimators=20, random_state=42)
        case "SVM":
            classifier = svm.SVC(C = 1.0, gamma = 0.001)
        case _:
            raise ValueError("Unsupported model")

    classifier.fit(train_data, train_labels)
    predicted = classifier.predict(test_data)

    expected = test_labels
    print(15*"-")
    print(model,"\n random seed:",randseed," test_rate:",test_rate)
    print('Accuracy:',accuracy_score(expected, predicted))
    print('Confusion matrix:\n', confusion_matrix(expected, predicted))
    print("각 클래스의 점수")
    print("precision:",precision_score(expected, predicted, average=None))
    print("recall:",recall_score(expected, predicted, average=None))
    print("f-measure:",f1_score(expected, predicted, average=None))
    print("\n 평균점수")
    print("precision:",precision_score(expected, predicted, average='macro'))
    print("recall:",recall_score(expected, predicted, average='macro'))
    print("f-measure:",f1_score(expected, predicted, average='macro'))
    print(15*"-")

"""바이너리의 경우에도 유의미한 정확도를  보이지 않아 유사한 의료기관종별을 합치기로 하였다.


상급종합병원 36
종합병원 48
병원 50
요양병원 50
정신병원 33
의원 25
치과병원 27
보건의료원 23
한방병원 44

한의원 1
보건소 1
보건지소 1
보건진료소 1
치과의원 1

로 구성되었다.


(한방)병원: 병상 수가 30 이상(한방은 20)
종합병원: 병상수 100이상
의원: 병상수 30미만 입원치료 불가
치과병원:병상 조건없음(시설 요구사항이 다름)

그러므로 다음과 같이 합치기로 결정
1. 대형 병원 및 종합 병원 217
상급종합병원 36
종합병원 48
병원 50
요양병원 50
정신병원 33
이 그룹은 대규모 병원이며 다양한 진료를 지원

2. 소규모 및 전문 병원 124
의원 25
치과병원 27
치과의원 27
한방병원 44
한의원 1
이 그룹은 소규모 병원이나 특정 진료에 특화된 병원이며 외래진료나 특정 전문분야를 담당하는 기관이다. 또한 한방병원을 제외하면 입원이 안된다.

3. 공공 의료기관 26
보건의료원 23
보건소 1
보건지소 1
보건진료소 1
이 그룹은 공공 부문에서 운영하는 의료기관이다.

병원, 의원, 공공기관으로 분류하고 싶었으나 이럴경우 병원 카테고리에 너무 많은 데이터가 편중되므로 이와같이 분류하였다.
"""

# 자료구조
import pandas as pd
import numpy as np
# normalize
from sklearn.preprocessing import StandardScaler
# 분류기
from sklearn import tree, ensemble, svm
# 분류 성능평가용
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.metrics import precision_score, recall_score, f1_score
# 데이터 시각화용
from matplotlib import pyplot as plt
from sklearn.decomposition import PCA
import seaborn as sns
from mpl_toolkits.mplot3d import Axes3D
from matplotlib.colors import ListedColormap

# 파라미터 조정
test_rate =0.8
randseed = 42
np.random.seed(randseed)
cri = ","
file_path = r"./data.csv"
# 분류 옵션
modellist=["decision tree","random forest","adaboost","SVM"]
# 스케일조정옵션
isScaling = True


# 파일에서 데이터 로드
# file_path, cri -> df
match(file_path.split(".")[-1].lower()):
    case "txt":
        df = pd.read_csv(file_path, delimiter=cri, skipinitialspace=True)
    case "csv":
        df = pd.read_csv(file_path, delimiter=cri, skipinitialspace=True)
    case "xlsx":
        df = pd.read_excel(file_path)
    case _:
        raise ValueError("Unsupported file type")


# 맞춤데이터 편집
# 진료년도, 진료과목 삭제
df = df.drop(df.columns[[0, 2]], axis=1)
# 의료기관의 라벨을 합치는 작업+ 의료기관종별 라벨을 0부터 시작하는 정수형 라벨로 변경
mapping = {
    '상급종합병원': 0,
    '종합병원': 0,
    '병원': 0,
    '요양병원': 0,
    '정신병원': 0,
    '의원': 1,
    '치과병원': 1,
    '치과의원': 1,
    '한방병원': 1,
    '한의원': 1,
    '보건의료원': 2,
    '보건소': 2,
    '보건지소': 2,
    '보건진료소': 2
}
df['의료기관종별'] = df['의료기관종별'].map(mapping)


# 데이터 형태 파악
print(df.head())
pca = PCA(n_components=3)
X_pca = pca.fit_transform(df.iloc[:,1:])
lab = df.iloc[:,0]
fig = plt.figure(figsize=(10, 7))
ax = fig.add_subplot(111, projection='3d')
custom_colors = ['red', 'blue','green']
cmap = ListedColormap(custom_colors)
scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=lab, cmap=cmap, alpha=0.7)
ax.set_xlabel("PCA Component 1")
ax.set_ylabel("PCA Component 2")
ax.set_zlabel("PCA Component 3")
ax.set_title("3D PCA Visualization")


# normalize
if isScaling:
    features = df.drop(df.columns[0], axis=1)
    scaler = StandardScaler()
    scaled_features = scaler.fit_transform(features)
    df_scaled = pd.DataFrame(scaled_features, columns=features.columns)
    df_scaled['의료기관종별'] = df['의료기관종별']
    print("Scaled data\n",df_scaled.head())
    df = df_scaled
else:
    # 노말라이즈를 하지 않아도 라벨이 마지막 열로 가게 조정
    tmp= df['의료기관종별']
    df = df.drop('의료기관종별', axis=1)
    df['의료기관종별'] = tmp


# 섞고 test data 분리
# df, test_rate -> train_data, test_data,train_size
data_size = len(df)
train_size = int(test_rate * data_size)
indice = np.arange(data_size)
np.random.shuffle(indice)

train_idx = indice[:train_size]
test_idx = indice[train_size:]

train_data = df.iloc[train_idx]
test_data = df.iloc[test_idx]
#라벨 분리
train_labels = train_data.iloc[:,-1]
test_labels = test_data.iloc[:,-1]
train_data = train_data.iloc[:,:-1]
test_data = test_data.iloc[:,:-1]


# 분류실행/결과 출력까지
# train_data,train_labels ,test_data, modellist-> predicted
# predicted, test_labels -> 결과 출력
for model in modellist:
    match(model):
        case "decision tree":
            classifier = tree.DecisionTreeClassifier(max_depth=3)
        case "random forest":
            classifier = ensemble.RandomForestClassifier(n_estimators=20, random_state=42)
        case "adaboost":
            classifier = ensemble.AdaBoostClassifier(estimator = tree.DecisionTreeClassifier(max_depth=3),n_estimators=20, random_state=42)
        case "SVM":
            classifier = svm.SVC(C = 1.0, gamma = 0.001)
        case _:
            raise ValueError("Unsupported model")

    classifier.fit(train_data, train_labels)
    predicted = classifier.predict(test_data)

    expected = test_labels
    print(15*"-")
    print(model,"\n random seed:",randseed," test_rate:",test_rate)
    print('Accuracy:',accuracy_score(expected, predicted))
    print('Confusion matrix:\n', confusion_matrix(expected, predicted))
    print("각 클래스의 점수")
    print("precision:",precision_score(expected, predicted, average=None))
    print("recall:",recall_score(expected, predicted, average=None))
    print("f-measure:",f1_score(expected, predicted, average=None))
    print("\n 평균점수")
    print("precision:",precision_score(expected, predicted, average='macro'))
    print("recall:",recall_score(expected, predicted, average='macro'))
    print("f-measure:",f1_score(expected, predicted, average='macro'))
    print(15*"-")

# 자료구조
import pandas as pd
import numpy as np
# normalize
from sklearn.preprocessing import StandardScaler
# 분류기
from sklearn import tree, ensemble, svm
# 분류 성능평가용
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.metrics import precision_score, recall_score, f1_score
# 데이터 시각화용
from matplotlib import pyplot as plt
from sklearn.decomposition import PCA
import seaborn as sns
from mpl_toolkits.mplot3d import Axes3D
from matplotlib.colors import ListedColormap

# 파라미터 조정
test_rate =0.8
randseed = 42
np.random.seed(randseed)
cri = ","
file_path = r"./data.csv"
# 분류 옵션
modellist=["decision tree","random forest","adaboost","SVM"]
# 스케일조정옵션
isScaling = True


# 파일에서 데이터 로드
# file_path, cri -> df
match(file_path.split(".")[-1].lower()):
    case "txt":
        df = pd.read_csv(file_path, delimiter=cri, skipinitialspace=True)
    case "csv":
        df = pd.read_csv(file_path, delimiter=cri, skipinitialspace=True)
    case "xlsx":
        df = pd.read_excel(file_path)
    case _:
        raise ValueError("Unsupported file type")


# 맞춤데이터 편집
# 진료년도, 진료과목 삭제
df = df.drop(df.columns[[0, 2]], axis=1)
# 의료기관의 라벨을 합치는 작업+ 의료기관종별 라벨을 0부터 시작하는 정수형 라벨로 변경
mapping = {
    '상급종합병원': 0,
    '종합병원': 0,
    '병원': 0,
    '요양병원': 0,
    '정신병원': 0,
    '의원': 1,
    '치과병원': 0,
    '치과의원': 1,
    '한방병원': 0,
    '한의원': 1,
    '보건의료원': 1,
    '보건소': 1,
    '보건지소': 1,
    '보건진료소': 1
}
df['의료기관종별'] = df['의료기관종별'].map(mapping)


# 데이터 형태 파악
print(df.head())
pca = PCA(n_components=3)
X_pca = pca.fit_transform(df.iloc[:,1:])
lab = df.iloc[:,0]
fig = plt.figure(figsize=(10, 7))
ax = fig.add_subplot(111, projection='3d')
custom_colors = ['red', 'blue']
cmap = ListedColormap(custom_colors)
scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=lab, cmap=cmap, alpha=0.7)
ax.set_xlabel("PCA Component 1")
ax.set_ylabel("PCA Component 2")
ax.set_zlabel("PCA Component 3")
ax.set_title("3D PCA Visualization")


# normalize
if isScaling:
    features = df.drop(df.columns[0], axis=1)
    scaler = StandardScaler()
    scaled_features = scaler.fit_transform(features)
    df_scaled = pd.DataFrame(scaled_features, columns=features.columns)
    df_scaled['의료기관종별'] = df['의료기관종별']
    print("Scaled data\n",df_scaled.head())
    df = df_scaled
else:
    # 노말라이즈를 하지 않아도 라벨이 마지막 열로 가게 조정
    tmp= df['의료기관종별']
    df = df.drop('의료기관종별', axis=1)
    df['의료기관종별'] = tmp


# 섞고 test data 분리
# df, test_rate -> train_data, test_data,train_size
data_size = len(df)
train_size = int(test_rate * data_size)
indice = np.arange(data_size)
np.random.shuffle(indice)

train_idx = indice[:train_size]
test_idx = indice[train_size:]

train_data = df.iloc[train_idx]
test_data = df.iloc[test_idx]
#라벨 분리
train_labels = train_data.iloc[:,-1]
test_labels = test_data.iloc[:,-1]
train_data = train_data.iloc[:,:-1]
test_data = test_data.iloc[:,:-1]


# 분류실행/결과 출력까지
# train_data,train_labels ,test_data, modellist-> predicted
# predicted, test_labels -> 결과 출력
for model in modellist:
    match(model):
        case "decision tree":
            classifier = tree.DecisionTreeClassifier(max_depth=3)
        case "random forest":
            classifier = ensemble.RandomForestClassifier(n_estimators=20, random_state=42)
        case "adaboost":
            classifier = ensemble.AdaBoostClassifier(estimator = tree.DecisionTreeClassifier(max_depth=3),n_estimators=20, random_state=42)
        case "SVM":
            classifier = svm.SVC(C = 1.0, gamma = 0.001)
        case _:
            raise ValueError("Unsupported model")

    classifier.fit(train_data, train_labels)
    predicted = classifier.predict(test_data)

    expected = test_labels
    print(15*"-")
    print(model,"\n random seed:",randseed," test_rate:",test_rate)
    print('Accuracy:',accuracy_score(expected, predicted))
    print('Confusion matrix:\n', confusion_matrix(expected, predicted))
    print("각 클래스의 점수")
    print("precision:",precision_score(expected, predicted, average=None))
    print("recall:",recall_score(expected, predicted, average=None))
    print("f-measure:",f1_score(expected, predicted, average=None))
    print("\n 평균점수")
    print("precision:",precision_score(expected, predicted, average='macro'))
    print("recall:",recall_score(expected, predicted, average='macro'))
    print("f-measure:",f1_score(expected, predicted, average='macro'))
    print(15*"-")

# 자료구조
import pandas as pd
import numpy as np
# normalize
from sklearn.preprocessing import StandardScaler
# 분류기
from sklearn import tree, ensemble, svm
# 분류 성능평가용
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.metrics import precision_score, recall_score, f1_score
# 데이터 시각화용
from matplotlib import pyplot as plt
from sklearn.decomposition import PCA
import seaborn as sns
from mpl_toolkits.mplot3d import Axes3D
from matplotlib.colors import ListedColormap

# 파라미터 조정
test_rate =0.8
randseed = 42
np.random.seed(randseed)
cri = ","
file_path = r"./data.csv"
# 분류 옵션
modellist=["decision tree","random forest","adaboost","SVM"]
# 스케일조정옵션
isScaling = True


# 파일에서 데이터 로드
# file_path, cri -> df
match(file_path.split(".")[-1].lower()):
    case "txt":
        df = pd.read_csv(file_path, delimiter=cri, skipinitialspace=True)
    case "csv":
        df = pd.read_csv(file_path, delimiter=cri, skipinitialspace=True)
    case "xlsx":
        df = pd.read_excel(file_path)
    case _:
        raise ValueError("Unsupported file type")


# 맞춤데이터 편집
# 진료년도, 진료과목 삭제
df = df.drop(df.columns[[0, 2]], axis=1)
# 의료기관의 라벨을 합치는 작업+ 의료기관종별 라벨을 0부터 시작하는 정수형 라벨로 변경
mapping = {
    '상급종합병원': 0,
    '종합병원': 0,
    '병원': 0,
    '요양병원': 1,
    '정신병원': 1,
    '의원': 1,
    '치과병원': 1,
    '치과의원': 1,
    '한방병원': 1,
    '한의원': 1,
    '보건의료원': 1,
    '보건소': 1,
    '보건지소': 1,
    '보건진료소': 1
}
df['의료기관종별'] = df['의료기관종별'].map(mapping)


# 데이터 형태 파악
print(df.head())
pca = PCA(n_components=3)
X_pca = pca.fit_transform(df.iloc[:,1:])
lab = df.iloc[:,0]
fig = plt.figure(figsize=(10, 7))
ax = fig.add_subplot(111, projection='3d')
custom_colors = ['red', 'blue']
cmap = ListedColormap(custom_colors)
scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=lab, cmap=cmap, alpha=0.7)
ax.set_xlabel("PCA Component 1")
ax.set_ylabel("PCA Component 2")
ax.set_zlabel("PCA Component 3")
ax.set_title("3D PCA Visualization")


# normalize
if isScaling:
    features = df.drop(df.columns[0], axis=1)
    scaler = StandardScaler()
    scaled_features = scaler.fit_transform(features)
    df_scaled = pd.DataFrame(scaled_features, columns=features.columns)
    df_scaled['의료기관종별'] = df['의료기관종별']
    print("Scaled data\n",df_scaled.head())
    df = df_scaled
else:
    # 노말라이즈를 하지 않아도 라벨이 마지막 열로 가게 조정
    tmp= df['의료기관종별']
    df = df.drop('의료기관종별', axis=1)
    df['의료기관종별'] = tmp


# 섞고 test data 분리
# df, test_rate -> train_data, test_data,train_size
data_size = len(df)
train_size = int(test_rate * data_size)
indice = np.arange(data_size)
np.random.shuffle(indice)

train_idx = indice[:train_size]
test_idx = indice[train_size:]

train_data = df.iloc[train_idx]
test_data = df.iloc[test_idx]
#라벨 분리
train_labels = train_data.iloc[:,-1]
test_labels = test_data.iloc[:,-1]
train_data = train_data.iloc[:,:-1]
test_data = test_data.iloc[:,:-1]


# 분류실행/결과 출력까지
# train_data,train_labels ,test_data, modellist-> predicted
# predicted, test_labels -> 결과 출력
for model in modellist:
    match(model):
        case "decision tree":
            classifier = tree.DecisionTreeClassifier(max_depth=3)
        case "random forest":
            classifier = ensemble.RandomForestClassifier(n_estimators=20, random_state=42)
        case "adaboost":
            classifier = ensemble.AdaBoostClassifier(estimator = tree.DecisionTreeClassifier(max_depth=3),n_estimators=20, random_state=42)
        case "SVM":
            classifier = svm.SVC(C = 1.0, gamma = 0.001)
        case _:
            raise ValueError("Unsupported model")

    classifier.fit(train_data, train_labels)
    predicted = classifier.predict(test_data)

    expected = test_labels
    print(15*"-")
    print(model,"\n random seed:",randseed," test_rate:",test_rate)
    print('Accuracy:',accuracy_score(expected, predicted))
    print('Confusion matrix:\n', confusion_matrix(expected, predicted))
    print("각 클래스의 점수")
    print("precision:",precision_score(expected, predicted, average=None))
    print("recall:",recall_score(expected, predicted, average=None))
    print("f-measure:",f1_score(expected, predicted, average=None))
    print("\n 평균점수")
    print("precision:",precision_score(expected, predicted, average='macro'))
    print("recall:",recall_score(expected, predicted, average='macro'))
    print("f-measure:",f1_score(expected, predicted, average='macro'))
    print(15*"-")